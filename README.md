# Сервис для назначения ревьюеров на Pull Request

Привет! Это мой проект для стажировки в Авито. Задача была сделать небольшой сервис, который помогает командам разработки автоматизировать назначение ревьюеров на Pull Request (PR).

## Что это такое и как работает?

Основная идея — когда автор создает новый PR, сервис сам находит одного или двух свободных коллег из его же команды и назначает их в ревьюеры. Это экономит время, потому что не нужно вручную пинговать людей.

**Ключевые фичи:**
- **Автоназначение:** Создаешь PR — получаешь до двух ревьюеров автоматом.
- **Управление командами:** Можно создавать команды и добавлять в них участников.
- **Статус пользователя:** Можно сделать пользователя "неактивным", и ему перестанут приходить новые ревью.
- **Переназначение:** Если ревьюер занят, можно попросить сервис найти ему замену.
- **Просмотр своих ревью:** Любой пользователь может получить список PR, которые висят на нём.
- **Статистика:** Эндпоинт для получения статистики по назначениям ревьюверов и PR.
- **Массовая деактивация:** Безопасная деактивация всех пользователей команды с автоматическим переназначением открытых PR.

## Как запустить проект?

Проще всего поднять всё через Docker. Убедись, что у тебя установлены **Docker** и **Docker Compose**.

1.  Склонируй репозиторий.
2.  В терминале, в корне проекта, выполни команду:
    ```bash
    docker-compose up
    ```
3.  Готово! Сервис запустится и будет доступен на `http://localhost:8080`. База данных PostgreSQL тоже поднимется в контейнере, и все нужные таблицы создадутся сами (миграции применяются при старте).

Если хочешь остановить всё, нажми `Ctrl+C` в терминале, а потом выполни:
```bash
docker-compose down
```

## Как пользоваться API?

Я набросал несколько примеров с `curl`.



#### 1. Создать команду

```bash
curl -X POST http://localhost:8080/team/add \
  -H "Content-Type: application/json" \
  -d 
'{'
    "team_name": "backend-team",
    "members": [
      {"user_id": "user1", "username": "Anna", "is_active": true},
      {"user_id": "user2", "username": "Boris", "is_active": true},
      {"user_id": "user3", "username": "Clara", "is_active": true}
    ]
  }'
```

#### 2. Создать Pull Request

Допустим, `user1` (Anna) создаёт PR. Сервис найдёт двух ревьюеров из её команды (например, `user2` и `user3`).

```bash
curl -X POST http://localhost:8080/pullRequest/create \
  -H "Content-Type: application/json" \

  -d 
'{'
    "pull_request_id": "pr-awesome-feature",
    "pull_request_name": "New awesome feature",
    "author_id": "user1"
  }'
```

#### 3. Переназначить ревьюера

Допустим, `user2` (Boris) не может посмотреть PR. Попросим сервис найти ему замену.

```bash
curl -X POST http://localhost:8080/pullRequest/reassign \
  -H "Content-Type: application/json" \
  -d '{
    "pull_request_id": "pr-awesome-feature",
    "old_user_id": "user2"
  }'
```

#### 4. Получить статистику

Получить статистику по назначениям ревьюверов и PR:

```bash
curl -X GET http://localhost:8080/statistics
```

#### 5. Массовая деактивация команды

Деактивировать всех пользователей команды с автоматическим переназначением открытых PR:

```bash
curl -X POST http://localhost:8080/team/bulkDeactivate \
  -H "Content-Type: application/json" \
  -d '{
    "team_name": "backend-team"
  }'
```

## Мысли и решения в ходе разработки

В процессе были моменты, где нужно было принять решение. Вот некоторые из них:

-   **Случайный выбор ревьюера.** Использовал стандартный `math/rand`. Знаю, что для криптографии он не подходит, но для нашей задачи (просто выбрать случайного коллегу) его более чем достаточно. Можно было бы использовать `crypto/rand`, но это избыточно.

-   **Запуск миграций.** Чтобы всё поднималось одной командой `docker-compose up`, встроил запуск миграций прямо в код приложения. Сервис при старте сначала ждет, пока база данных станет доступна, накатывает миграции, и только потом запускается. Удобно! Не нужно помнить про отдельный шаг.

-   **Идемпотентность.** Операция мержа PR сделана идемпотентной. Если случайно отправить запрос на мерж дважды, ошибки не будет — сервис просто вернет уже смерженный PR. Это важно, потому что в реальной жизни могут быть повторные запросы из-за сетевых проблем или багов на фронте.

-   **Тесты.** Сначала думал делать unit-тесты с моками, но потом решил что для такого сервиса проще и полезнее сделать E2E тесты. Они проверяют реальное поведение и проще писать.

## Проблемы и сложности

В процессе разработки столкнулся с несколькими проблемами, которые пришлось решать:

### 1. Массовая деактивация команды

Самая интересная проблема была с массовой деактивацией. Сначала я делал так: находил ревьюера, которого нужно заменить, и искал замену в его команде. Но если деактивируешь всю команду, то в ней не останется активных пользователей для замены!

**Решение:** Стал искать замену в команде автора PR, а не в команде заменяемого ревьюера. Так работает правильно даже если деактивируешь всю команду - замену всегда можно найти в команде автора (если она не деактивируется одновременно).

### 2. Производительность массовой деактивации

В ТЗ было требование уложиться в 100мс для средних объемов данных. Сначала делал всё последовательно - для каждого PR отдельно искал замену, потом деактивировал. Это было медленно.

**Решение:** Оптимизировал - сначала собираю все открытые PR, которые нужно переназначить, потом делаю переназначения батчами, и только потом деактивирую пользователей. Получилось быстрее, укладывается в 100мс.

### 3. Запуск миграций при старте

Хотел чтобы всё поднималось одной командой `docker-compose up`, но миграции нужно было как-то запускать. Варианты были: отдельный скрипт, init-контейнер, или встроить в код.

**Решение:** Встроил запуск миграций прямо в код приложения. Сервис при старте ждет пока база станет доступна (делает несколько попыток), потом накатывает миграции, и только потом запускается. Удобно - не нужно помнить про отдельный шаг.

### 4. Идемпотентность merge

Сначала не подумал про идемпотентность. Если дважды отправить запрос на мерж, могла быть ошибка или дублирование.

**Решение:** Добавил проверку - если PR уже смержен, просто возвращаю его текущее состояние без ошибки. Это важно для реальной жизни, где могут быть повторные запросы.

### 5. Тестирование

Долго думал как тестировать. Unit-тесты с моками - сложно писать и поддерживать. E2E тесты - проще, но нужен запущенный сервис.

**Решение:** Выбрал E2E тесты. Они проверяют реальное поведение, проще писать, и для такого сервиса этого достаточно. Правда пришлось сделать так, чтобы тесты ждали пока сервис запустится (добавил проверку в TestMain).

### 6. Интеграционный тест

Когда писал интеграционный тест, сначала использовал body от merge запроса для проверки переназначения на смерженном PR. Получалась ошибка 400 вместо 409, потому что формат запроса был неправильный.

**Решение:** Исправил - теперь беру актуального ревьюера из смерженного PR и формирую правильный запрос для переназначения. Проверка статуса PR происходит первой, поэтому правильно возвращается ошибка 409.

## Дополнительные задания

Постарался выполнить все дополнительные задания из ТЗ:

1. **Эндпоинт статистики** (`GET /statistics`) - показывает сколько назначений у каждого пользователя и общую статистику по PR. Полезно для аналитики.

2. **Массовая деактивация** (`POST /team/bulkDeactivate`) - можно деактивировать всю команду сразу, при этом открытые PR автоматически переназначаются на других активных пользователей. Это было интересно реализовать - нужно было учесть что при деактивации команды все её участники становятся неактивными, поэтому замену ищу в команде автора PR, а не в команде заменяемого ревьюера.

3. **Нагрузочное тестирование** - написал скрипт в `load_test/main.go`, который проверяет все эндпоинты и смотрит, укладывается ли сервис в SLI. Результаты получились хорошие - сервис справляется с нагрузкой.

4. **Интеграционное тестирование** - написал тесты в `internal/e2e/`, которые проверяют сервис через реальный HTTP API. Есть отдельные тесты для каждого эндпоинта и один большой интеграционный тест, который проверяет весь flow целиком.

5. **Конфигурация линтера** - настроил `.golangci.yml` для проверки кода. Использовал стандартные правила, ничего экзотического.

## Тестирование

Написал тесты, которые проверяют сервис через реальный HTTP API. Все тесты работают с запущенным сервисом, так что перед запуском нужно поднять его через docker-compose.

```bash
# Сначала запусти сервис
docker-compose up

# В другом терминале запусти тесты
go test -v ./internal/e2e/...
```

### Что проверяют тесты?

Есть два типа тестов:

1. **Отдельные тесты** (`e2e_test.go`) - проверяют каждый эндпоинт по отдельности:
   - Health check
   - Создание и получение команд
   - Создание PR с автоназначением ревьюеров
   - Проверка что неактивные пользователи не назначаются
   - Мерж PR (включая проверку идемпотентности)
   - Переназначение ревьюера
   - Запрет переназначения на смерженных PR
   - Получение списка PR пользователя
   - Управление активностью пользователя
   - Получение статистики
   - Массовая деактивация команды

2. **Интеграционный тест** (`integration_test.go`) - один большой тест, который проверяет весь flow от начала до конца:
   - Создает команду с несколькими пользователями
   - Создает PR и проверяет что ревьюеры назначились
   - Переназначает ревьюера
   - Деактивирует пользователя
   - Мержит PR
   - Проверяет что на смерженном PR нельзя переназначить ревьюера
   - Получает статистику

Этот тест полезен тем, что проверяет не отдельные кусочки, а весь сценарий целиком - как будто реальный пользователь работает с сервисом. Если что-то сломается, сразу видно на каком этапе.

Код тестов я старался писать просто - без лишнего логирования и сложных конструкций. Просто проверяю что всё работает как надо.

### Ручное тестирование

Для тестирования всех сценариев используйте команды из файла `TEST_COMMANDS.md`.

### Запуск нагрузочного тестирования

Написал простой скрипт для нагрузочного тестирования, который проверяет все эндпоинты и смотрит, укладывается ли сервис в требования из ТЗ.

```bash
# Сначала запусти сервис
docker-compose up

# В другом терминале запусти тесты
cd load_test
go run main.go
```

Скрипт сам создаст тестовые данные (команды, пользователи, PR) и прогонит три уровня нагрузки. В конце выведет отчет с метриками и проверкой SLI.


## Линтинг

Проект использует `golangci-lint` для проверки кода. Конфигурация находится в `.golangci.yml`.

Для запуска линтера:
```bash
golangci-lint run ./...
```

## Стек технологий

-   **Язык:** Go
-   **Веб-фреймворк:** Gin (очень понравился, простой и быстрый)
-   **База данных:** PostgreSQL
-   **Миграции:** `golang-migrate`
-   **Контейнеры:** Docker
-   **Линтинг:** golangci-lint

## Производительность

Сервис справляется с требованиями из ТЗ:
- ✅ RPS: 5 (поддерживается)
- ✅ SLI времени ответа: <300ms (среднее получается около 45ms)
- ✅ SLI успешности: >99.9% (при базовой нагрузке получается 100%)

Подробные результаты нагрузочного тестирования см. в `LOAD_TEST_RESULTS.md`.

Спасибо за интерес к моему проекту!